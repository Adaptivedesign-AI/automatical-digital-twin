import gradio as gr
import os
import requests

# è¯»å–ç³»ç»Ÿæç¤ºè¯
with open("student_prompt.txt", "r") as f:
    system_prompt = f.read().strip()

student_avatar_url = "https://raw.githubusercontent.com/Adaptivedesign-AI/Digital-twin-003/main/image.png"

# âœ… æ‰€æœ‰ OpenRouter ä¸Šå¯ç”¨çš„å¤§æ¨¡å‹ï¼ˆæˆªè‡³ 2025-05ï¼‰
model_choices = [
    "mistralai/mistral-7b-instruct",
    "openchat/openchat-3.5-1210",
    "google/gemma-7b-it",
    "meta-llama/llama-2-7b-chat",
    "zero-one-ai/Yi-1.5-9B-Chat",
    "nousresearch/nous-hermes-2-mistral",
    "meta-llama/Meta-Llama-3-8B-Instruct"
]

# Improve error handling in chat_with_student003 function
def chat_with_student003(message, history, selected_model):
    messages = [{"role": "system", "content": system_prompt}]
    for user_msg, bot_reply in history:
        messages.append({"role": "user", "content": user_msg})
        messages.append({"role": "assistant", "content": bot_reply})
    messages.append({"role": "user", "content": message})
    
    headers = {
        "Authorization": f"Bearer {os.environ['OPENROUTER_API_KEY']}",
        "HTTP-Referer": "https://chat-with-003-openrouter.onrender.com",
        "X-Title": "Digital-Twin-003",
        "Content-Type": "application/json"
    }
    
    payload = {
        "model": selected_model,
        "messages": messages,
        "temperature": 0.7
    }
    
    try:
        print(f"\nğŸ§ª æ­£åœ¨è°ƒç”¨æ¨¡å‹: {selected_model}")
        response = requests.post(
            "https://openrouter.ai/api/v1/chat/completions",
            headers=headers,
            json=payload,
            timeout=60
        )
        
        print(f"ğŸ”¢ çŠ¶æ€ç : {response.status_code}")
        
        # è¯¦ç»†è®°å½•å“åº”å†…å®¹ä»¥ä¾¿è°ƒè¯•
        if response.status_code != 200:
            print(f"âŒ é”™è¯¯å“åº”: {response.text}")
            return f"æ¨¡å‹ {selected_model} è¿”å›é”™è¯¯: HTTP {response.status_code}"
            
        result = response.json()
        print(f"ğŸ“¨ å“åº”æˆåŠŸ: {result.keys()}")
        
        # æ£€æŸ¥å“åº”æ ¼å¼æ˜¯å¦ç¬¦åˆé¢„æœŸ
        if "choices" not in result or len(result["choices"]) == 0:
            print(f"âŒ å“åº”æ ¼å¼é”™è¯¯: {result}")
            return f"æ¨¡å‹ {selected_model} è¿”å›äº†æ„å¤–çš„å“åº”æ ¼å¼"
            
        return result["choices"][0]["message"]["content"].strip()
        
    except requests.exceptions.Timeout:
        print("âŒ è¯·æ±‚è¶…æ—¶")
        return f"æ¨¡å‹ {selected_model} è¯·æ±‚è¶…æ—¶ï¼Œè¯·ç¨åå†è¯•"
    except requests.exceptions.RequestException as e:
        print(f"âŒ è¯·æ±‚å¼‚å¸¸: {e}")
        return f"ä¸æ¨¡å‹ {selected_model} é€šä¿¡æ—¶å‡ºç°ç½‘ç»œé—®é¢˜"
    except Exception as e:
        print(f"âŒ å…¶ä»–å¼‚å¸¸: {e}")
        return f"ä½¿ç”¨æ¨¡å‹ {selected_model} æ—¶å‡ºç°é”™è¯¯: {str(e)}"
        
# âœ… æ„å»º Gradio ç•Œé¢
with gr.Blocks() as demo:
    gr.Markdown("## ğŸ§  Talk to Student003 â€” A Digital Twin")
    gr.Markdown("Compare different open LLMs in a unified interface.")

    with gr.Row():
        model_dropdown = gr.Dropdown(
            choices=model_choices,
            label="Choose LLM Model",
            value="mistralai/mistral-7b-instruct"
        )

    chatbot = gr.Chatbot(label="Conversation", avatar_images=(None, student_avatar_url))
    msg = gr.Textbox(placeholder="Type your message here...")
    clear = gr.Button("Clear Conversation")

    def respond(message, chat_history, selected_model):
        bot_message = chat_with_student003(message, chat_history, selected_model)
        chat_history.append((message, bot_message))
        return "", chat_history

    msg.submit(respond, [msg, chatbot, model_dropdown], [msg, chatbot])
    clear.click(lambda: [], None, chatbot, queue=False)

# âœ… å¯åŠ¨æœåŠ¡å™¨ï¼ˆå…¼å®¹ Renderï¼‰
if __name__ == "__main__":
    demo.queue(api_open=True).launch(server_name="0.0.0.0", server_port=int(os.environ.get("PORT", 7860)))
