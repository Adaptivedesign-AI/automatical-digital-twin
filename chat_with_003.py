import gradio as gr
import os
import requests
import json
import time

# è¯»å–ç³»ç»Ÿæç¤ºè¯
with open("student_prompt.txt", "r") as f:
    system_prompt = f.read().strip()

student_avatar_url = "https://raw.githubusercontent.com/Adaptivedesign-AI/Digital-twin-003/main/image.png"

# æ›´æ–°ä¸ºæœ€æ–°çš„æ¨¡å‹ID (2025-05çš„å¯ç”¨æ¨¡å‹)
model_choices = [
    "mistralai/mistral-7b-instruct",           # è¿™ä¸ªå·¥ä½œæ­£å¸¸
    "openchat/openchat-3.5-0106",              # ä½¿ç”¨æ›´æ–°ç‰ˆæœ¬çš„OpenChat
    "google/gemma-3-27b-it:free",              # æ›´æ–°ä¸ºGemma 3 (å…è´¹å±‚çº§)
    "meta-llama/llama-3-8b-instruct",          # æ›´æ–°ä¸ºLlama 3
    "01-ai/yi-1.5-9b-chat",                    # æ­£ç¡®æ ¼å¼
    "mistralai/mixtral-8x7b-instruct",         # æ›¿ä»£æˆ‘ä»¬èƒ½æ‰¾åˆ°çš„æ¨¡å‹
    "anthropic/claude-3-haiku-20240307"        # æ·»åŠ å¯é çš„æ›¿ä»£æ¨¡å‹
]

def chat_with_student003(message, history, selected_model):
    messages = [{"role": "system", "content": system_prompt}]
    
    for user_msg, bot_reply in history:
        messages.append({"role": "user", "content": user_msg})
        messages.append({"role": "assistant", "content": bot_reply})
    
    messages.append({"role": "user", "content": message})
    
    headers = {
        "Authorization": f"Bearer {os.environ['OPENROUTER_API_KEY']}",
        "HTTP-Referer": "https://chat-with-003-openrouter.onrender.com",
        "X-Title": "Digital-Twin-003",
        "Content-Type": "application/json"
    }
    
    # æ·»åŠ é”™è¯¯é‡è¯•æœºåˆ¶
    max_retries = 2
    retry_delay = 1.5
    
    for attempt in range(max_retries + 1):
        try:
            print(f"\nğŸ§ª å°è¯•ä½¿ç”¨æ¨¡å‹: {selected_model} (å°è¯• {attempt+1}/{max_retries+1})")
            
            # æ„å»ºè¯·æ±‚è´Ÿè½½
            payload = {
                "model": selected_model,
                "messages": messages,
                "temperature": 0.7,
                "max_tokens": 1000,
                # æ·»åŠ æ¨¡å‹å›é€€é€‰é¡¹ï¼Œæé«˜å¯é æ€§
                "models": [selected_model, "mistralai/mistral-7b-instruct"],
                # æ›´çµæ´»çš„æä¾›å•†è®¾ç½®
                "provider": {
                    "data_collection": "allow",  # å…è®¸æ•°æ®æ”¶é›†å¢åŠ å¯ç”¨æ€§
                    "require_parameters": False  # å…è®¸éƒ¨åˆ†å‚æ•°ä¸æ”¯æŒ
                }
            }
            
            response = requests.post(
                "https://openrouter.ai/api/v1/chat/completions",
                headers=headers,
                json=payload,
                timeout=30  # è¾ƒçŸ­è¶…æ—¶ï¼Œå¯ç”¨é‡è¯•æœºåˆ¶
            )
            
            if response.status_code == 200:
                result = response.json()
                return result["choices"][0]["message"]["content"].strip()
            
            # æœ‰äº›é”™è¯¯å¯ä»¥é‡è¯•
            if response.status_code in [429, 500, 502, 503, 504] and attempt < max_retries:
                error_msg = f"è¯·æ±‚å¤±è´¥ (HTTP {response.status_code}): {response.text} - å°†åœ¨ {retry_delay}ç§’åé‡è¯•..."
                print(error_msg)
                time.sleep(retry_delay)
                retry_delay *= 2  # æŒ‡æ•°é€€é¿
                continue
                
            # è§£æé”™è¯¯å“åº”
            error_text = "æœªçŸ¥é”™è¯¯"
            try:
                error_data = response.json()
                if "error" in error_data and "message" in error_data["error"]:
                    error_text = error_data["error"]["message"]
            except:
                error_text = response.text[:100] + "..." if len(response.text) > 100 else response.text
                
            return f"æ¨¡å‹ {selected_model} è¿”å›é”™è¯¯: HTTP {response.status_code} - {error_text}"
            
        except Exception as e:
            if attempt < max_retries:
                print(f"âŒ å¼‚å¸¸: {str(e)} - å°†åœ¨ {retry_delay}ç§’åé‡è¯•...")
                time.sleep(retry_delay)
                retry_delay *= 2  # æŒ‡æ•°é€€é¿
                continue
            return f"ä½¿ç”¨æ¨¡å‹ {selected_model} æ—¶å‡ºç°é”™è¯¯: {str(e)}"

# Gradio ç•Œé¢æ„å»º
with gr.Blocks() as demo:
    gr.Markdown("## ğŸ§  Talk to Student003 â€” A Digital Twin")
    gr.Markdown("Compare different open LLMs in a unified interface.")
    
    with gr.Row():
        model_dropdown = gr.Dropdown(
            choices=model_choices,
            label="Choose LLM Model",
            value="mistralai/mistral-7b-instruct"
        )
    
    chatbot = gr.Chatbot(label="Conversation", avatar_images=(None, student_avatar_url))
    msg = gr.Textbox(placeholder="Type your message here...")
    clear = gr.Button("Clear Conversation")
    
    # æ·»åŠ æ¨¡å‹çŠ¶æ€æŒ‡ç¤ºå™¨
    model_status = gr.Markdown("ğŸ’¡ **æç¤º**: å¦‚æœæŸä¸ªæ¨¡å‹æ— æ³•ä½¿ç”¨ï¼Œè¯·å°è¯•å…¶ä»–æ¨¡å‹")
    
    def respond(message, chat_history, selected_model):
        if not message.strip():
            return "", chat_history
        
        bot_message = chat_with_student003(message, chat_history, selected_model)
        chat_history.append((message, bot_message))
        return "", chat_history
    
    msg.submit(respond, [msg, chatbot, model_dropdown], [msg, chatbot])
    clear.click(lambda: [], None, chatbot, queue=False)

# å¯åŠ¨æœåŠ¡å™¨
if __name__ == "__main__":
    demo.queue(api_open=True).launch(server_name="0.0.0.0", server_port=int(os.environ.get("PORT", 7860)))
